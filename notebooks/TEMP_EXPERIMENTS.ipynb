{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy \n",
    "\n",
    "import textwrap\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # suppress sklearn deprecation warnings for now..\n",
    "try:\n",
    "    import graphviz\n",
    "except ModuleNotFoundError as e:\n",
    "    import pip\n",
    "    pip.main([\"install\",\"graphviz\"])\n",
    "    import graphviz\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "root_path = root_path = os.path.realpath('../../../..')\n",
    "# print(root_path)\n",
    "try:\n",
    "    import causaltune\n",
    "except ModuleNotFoundError:\n",
    "    sys.path.append(os.path.join(root_path, \"causaltune\"))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from causaltune import CausalTune\n",
    "from causaltune.data_utils import CausalityDataset\n",
    "from causaltune.datasets import generate_synthetic_data\n",
    "\n",
    "# Import linear synthetic data creation\n",
    "from causaltune.datasets import generate_linear_synthetic_data\n",
    "from causaltune.models.passthrough import passthrough_model\n",
    "from causaltune.datasets import load_dataset, save_dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# set a few params\n",
    "metrics = [\n",
    "    # \"frobenius_norm\",\n",
    "     \"psw_frobenius_norm\",\n",
    "    # \"prob_erupt\",\n",
    "    #\"policy_risk\",\n",
    "    # \"codec\",\n",
    "    # \"energy_distance\", \n",
    "    # \"psw_energy_distance\",\n",
    "    # \"bite\"\n",
    "    ]\n",
    "\n",
    "iv_metrics = [\n",
    "    # \"energy_distance\",\n",
    "    # \"codec\", \n",
    "    \"frobenius_norm\",                      \n",
    "    ]\n",
    "\n",
    "estimator_list = [\n",
    "            \"Dummy\",\n",
    "            \"SparseLinearDML\",\n",
    "            \"ForestDRLearner\",\n",
    "            \"TransformedOutcome\",\n",
    "            \"CausalForestDML\",\n",
    "            \".LinearDML\",\n",
    "            \"DomainAdaptationLearner\",\n",
    "            \"SLearner\",\n",
    "            \"XLearner\",\n",
    "            \"TLearner\",\n",
    "            #\"Ortho\"        \n",
    "             ] \n",
    "\n",
    "iv_estimator_list = [\n",
    "                 'iv.econml.iv.dr.LinearDRIV', \n",
    "                 #'iv.econml.iv.dml.OrthoIV', \n",
    "                 'iv.econml.iv.dml.DMLIV',\n",
    "                 'iv.econml.iv.dr.SparseLinearDRIV',\n",
    "                 'iv.econml.iv.dr.LinearIntentToTreatDRIV'\n",
    "                 ] \n",
    "\n",
    "\n",
    "# More Parameters\n",
    "n_runs = 1\n",
    "num_samples = -1\n",
    "\n",
    "test_size = 0.33 # equal train,val,test\n",
    "\n",
    "time_budget = None\n",
    "components_time_budget = 10\n",
    "\n",
    "propensity_model='dummy'\n",
    "\n",
    "filename_out = \"\"\n",
    "out_dir = \"PSW_FROBENIUS_TESTS\"\n",
    "\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize an empty dictionary to store the loaded datasets\n",
    "data_sets = {}\n",
    "\n",
    "# List of dataset names and file paths\n",
    "dataset_names = ['Linear_RCT']#, 'NonLinear_KC', 'NonLinear_KCKP', 'NonLinear_IV', \n",
    "                 #'Linear_RCT', 'Linear_KC', 'Linear_KCKP', 'Linear_IV']\n",
    "file_paths = [f\"RunDatasets/Small/{name}.pkl\" for name in dataset_names]\n",
    "\n",
    "# Loop through dataset names and file paths to load each dataset\n",
    "for name, file_path in zip(dataset_names, file_paths):\n",
    "    data_sets[name] = load_dataset(file_path)\n",
    "\n",
    "# Optionally, print the keys of the dictionary to verify successful loading\n",
    "print(f\"Loaded datasets: {list(data_sets.keys())}\")\n",
    "\n",
    "\n",
    "\n",
    "for dataset_name, cd in data_sets.items():\n",
    "    \n",
    "    for i_run in range(1,n_runs+1):\n",
    "        \n",
    "        cd_i = copy.deepcopy(cd)\n",
    "        train_df, test_df = train_test_split(cd_i.data, test_size=test_size)\n",
    "        test_df = test_df.reset_index(drop=True)\n",
    "        cd_i.data = train_df\n",
    "        \n",
    "        #for metric in metrics:\n",
    "        for metric in metrics:\n",
    "            \n",
    "            #propensity_model=passthrough_model(\n",
    "            #    cd.propensity_modifiers, include_control=False\n",
    "            #)\n",
    "            ct = CausalTune(\n",
    "                metric=metric,\n",
    "                estimator_list=estimator_list,\n",
    "\n",
    "                num_samples = num_samples,\n",
    "                time_budget=time_budget,\n",
    "                components_time_budget=components_time_budget,               \n",
    "                \n",
    "                metrics_to_report=metrics,\n",
    "                verbose=1,\n",
    "                components_verbose=1,\n",
    "                store_all_estimators=True,\n",
    "\n",
    "                #propensity_model=propensity_model,\n",
    "            )\n",
    "            \n",
    "            ct.fit(\n",
    "                data=cd_i,\n",
    "                treatment=\"treatment\",\n",
    "                outcome=\"outcome\",\n",
    "            )\n",
    "\n",
    "            \n",
    "            # compute relevant scores (skip newdummy)\n",
    "            datasets = {\"train\": ct.train_df, \"validation\": ct.test_df, \"test\": test_df}\n",
    "            # get scores on train,val,test for each trial, \n",
    "            # sort trials by validation set performance\n",
    "            # assign trials to estimators\n",
    "            estimator_scores = {est: [] for est in ct.scores.keys() if \"NewDummy\" not in est}\n",
    "            for trial in ct.results.trials:\n",
    "                # estimator name:\n",
    "                estimator_name = trial.last_result[\"estimator_name\"]\n",
    "                if  trial.last_result[\"estimator\"]:\n",
    "                    estimator = trial.last_result[\"estimator\"]\n",
    "                    scores = {}\n",
    "                    for ds_name, df in datasets.items():\n",
    "                        scores[ds_name] = {}\n",
    "                        # make scores\n",
    "                        est_scores = ct.scorer.make_scores(\n",
    "                            estimator,\n",
    "                            df,\n",
    "                            metrics_to_report=ct.metrics_to_report,\n",
    "                        )\n",
    "                        \n",
    "                        # add cate:\n",
    "                        scores[ds_name][\"CATE_estimate\"] = estimator.estimator.effect(df)\n",
    "                        # add ground truth for convenience\n",
    "                        scores[ds_name][\"CATE_groundtruth\"] = df[\"true_effect\"]\n",
    "                        scores[ds_name][metric] = est_scores[metric]\n",
    "                    scores['optimization_score'] = trial.last_result.get('optimization_score')\n",
    "\n",
    "                    estimator_scores[estimator_name].append(scores)\n",
    "\n",
    "\n",
    "            # sort trials by validation performance\n",
    "            for k in estimator_scores.keys():\n",
    "                estimator_scores[k] = sorted(\n",
    "                    estimator_scores[k],\n",
    "                    key=lambda x: x[\"validation\"][metric],\n",
    "                    reverse=False if metric in [\"energy_distance\", \n",
    "                                                \"psw_energy_distance\", \n",
    "                                                \"codec\", \n",
    "                                                \"frobenius_norm\", \n",
    "                                                \"psw_frobenius_norm\",\n",
    "                                                \"policy_risk\"] else True,\n",
    "                )\n",
    "            results = {\n",
    "                \"best_estimator\": ct.best_estimator,\n",
    "                \"best_config\": ct.best_config,\n",
    "                \"best_score\": ct.best_score,\n",
    "                \"optimised_metric\": metric,\n",
    "                \"scores_per_estimator\": estimator_scores,\n",
    "            }\n",
    "\n",
    "\n",
    "            with open(f\"{out_dir}/{filename_out}_{metric}_run_{i_run}_{dataset_name}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causaltune-paper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
