{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34f30c6",
   "metadata": {},
   "source": [
    "## FLAML for hp optimisation and model selection\n",
    "We use FLAML twice, first to find the best component model for each estimator, and then to optimise the estimators themselves and choose the best estimator. Here we show how it's done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d7480f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # suppress sklearn deprecation warnings for now.. \n",
    "\n",
    "# the below checks for whether we run dowhy and auto-causality from source\n",
    "root_path = root_path = os.path.realpath('../..')\n",
    "try: \n",
    "    import auto_causality\n",
    "except ModuleNotFoundError:\n",
    "    sys.path.append(os.path.join(root_path, \"auto-causality\"))\n",
    "    \n",
    "try:\n",
    "    import dowhy\n",
    "except ModuleNotFoundError:\n",
    "    sys.path.append(os.path.join(root_path, \"dowhy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8de5d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from auto_causality import AutoCausality\n",
    "from auto_causality.datasets import synth_ihdp, preprocess_dataset\n",
    "from auto_causality.scoring import ate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90d234a",
   "metadata": {},
   "source": [
    "### Model fitting & scoring\n",
    "Here we fit a (selection of) model(s) to the data and score them with the ERUPT metric on held-out data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68c4d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-14 14:29:27] {2145} WARNING - Time taken to find the best model is 88% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n",
      "[flaml.tune.tune: 03-14 14:29:27] {326} WARNING - Using CFO for search. To use BlendSearch, run: pip install flaml[blendsearch]\n",
      "[flaml.tune.tune: 03-14 14:29:27] {447} INFO - trial 1 config: {'fit_cate_intercept': 1, 'mc_iters': 8}\n",
      "[flaml.tune.tune: 03-14 14:38:18] {108} INFO - result: {'erupt': 6.584295597076416, 'qini': -0.049453029342096826, 'auc': 0.5369621767422776, 'ate': 3.658294605930205, 'r_score': 0.12867089289821954, 'estimator': <dowhy.causal_estimator.CausalEstimate object at 0x000002440D372BE0>, 'scores': {'estimator_name': 'backdoor.econml.dml.LinearDML', 'train': {'erupt': 6.3989696876413635, 'qini': 0.04337000423617723, 'auc': 0.5505174778691536, 'r_score': 0.06628259380834034, 'ate': 3.730222743597676, 'intrp': <econml.cate_interpreter._interpreters.SingleTreeCateInterpreter object at 0x000002440F2673A0>, 'values':      treated  y_factual         p  policy   weights\n",
      "0        0.0   4.592250  0.190955    True  5.226022\n",
      "1        0.0   1.665200  0.190955    True  0.000000\n",
      "2        0.0   6.420538  0.190955    True       NaN\n",
      "3        0.0   3.573685  0.190955    True  0.000000\n",
      "4        1.0   5.650913  0.190955    True  0.000000\n",
      "..       ...        ...       ...     ...       ...\n",
      "592      0.0   3.552017  0.190955    True       NaN\n",
      "593      0.0   2.573101  0.190955    True  0.000000\n",
      "594      0.0   4.109239  0.190955    True  0.000000\n",
      "595      0.0   2.947198  0.190955    True       NaN\n",
      "596      0.0   1.950411  0.190955    True  0.000000\n",
      "\n",
      "[597 rows x 5 columns]}, 'validation': {'erupt': 6.584295597076416, 'qini': -0.049453029342096826, 'auc': 0.5369621767422776, 'r_score': 0.12867089289821954, 'ate': 3.658294605930205, 'intrp': <econml.cate_interpreter._interpreters.SingleTreeCateInterpreter object at 0x000002440F267910>, 'values':      treated  y_factual         p  policy  weights\n",
      "0        0.0   0.810612  0.166667    True      NaN\n",
      "1        1.0   5.033105  0.166667    True      NaN\n",
      "2        1.0   6.917381  0.166667    True      0.0\n",
      "3        0.0   1.480015  0.166667    True      NaN\n",
      "4        0.0   1.792748  0.166667    True      NaN\n",
      "..       ...        ...       ...     ...      ...\n",
      "145      0.0   6.747466  0.166667    True      NaN\n",
      "146      0.0   5.030282  0.166667    True      NaN\n",
      "147      0.0   2.340850  0.166667    True      NaN\n",
      "148      0.0   1.935907  0.166667    True      0.0\n",
      "149      0.0   2.560395  0.166667    True      NaN\n",
      "\n",
      "[150 rows x 5 columns]}}, 'training_iteration': 0, 'config': {'fit_cate_intercept': 1, 'mc_iters': 8}, 'config/fit_cate_intercept': 1, 'config/mc_iters': 8, 'experiment_tag': 'exp', 'time_total_s': 530.5217385292053}\n",
      "[flaml.tune.tune: 03-14 14:38:18] {326} WARNING - Using CFO for search. To use BlendSearch, run: pip install flaml[blendsearch]\n",
      "[flaml.tune.tune: 03-14 14:38:18] {447} INFO - trial 1 config: {'fit_cate_intercept': 1, 'mc_iters': 8, 'n_alphas': 777, 'n_alphas_cov': 7, 'tol': 0.0060376, 'max_iter': 1500}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Estimator: backdoor.econml.dml.LinearDML\n",
      " erupt (validation): 6.584296\n",
      " qini (validation): -0.049453\n",
      " auc (validation): 0.536962\n",
      " ate (validation): 3.658295\n",
      " r_score (validation): 0.128671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.tune.tune: 03-14 14:46:20] {108} INFO - result: {'erupt': 6.584295597076416, 'qini': -0.06667256513466772, 'auc': 0.5370085583714438, 'ate': 3.540848670550587, 'r_score': 0.11658919330570794, 'estimator': <dowhy.causal_estimator.CausalEstimate object at 0x000002440D6039D0>, 'scores': {'estimator_name': 'backdoor.econml.dml.SparseLinearDML', 'train': {'erupt': 6.399111252082019, 'qini': 0.032526931855987026, 'auc': 0.5513643707984703, 'r_score': 0.06253219070216098, 'ate': 3.5911303555805953, 'intrp': <econml.cate_interpreter._interpreters.SingleTreeCateInterpreter object at 0x000002440FC4B880>, 'values':      treated  y_factual         p  policy   weights\n",
      "0        0.0   4.592250  0.190955    True  5.236842\n",
      "1        0.0   1.665200  0.190955    True  0.000000\n",
      "2        0.0   6.420538  0.190955    True       NaN\n",
      "3        0.0   3.573685  0.190955    True  0.000000\n",
      "4        1.0   5.650913  0.190955    True  0.000000\n",
      "..       ...        ...       ...     ...       ...\n",
      "592      0.0   3.552017  0.190955    True       NaN\n",
      "593      0.0   2.573101  0.190955    True  0.000000\n",
      "594      0.0   4.109239  0.190955    True  0.000000\n",
      "595      0.0   2.947198  0.190955    True       NaN\n",
      "596      0.0   1.950411  0.190955    True  0.000000\n",
      "\n",
      "[597 rows x 5 columns]}, 'validation': {'erupt': 6.584295597076416, 'qini': -0.06667256513466772, 'auc': 0.5370085583714438, 'r_score': 0.11658919330570794, 'ate': 3.540848670550587, 'intrp': <econml.cate_interpreter._interpreters.SingleTreeCateInterpreter object at 0x000002440FC4BDF0>, 'values':      treated  y_factual         p  policy  weights\n",
      "0        0.0   0.810612  0.166667    True      NaN\n",
      "1        1.0   5.033105  0.166667    True      NaN\n",
      "2        1.0   6.917381  0.166667    True      0.0\n",
      "3        0.0   1.480015  0.166667    True      NaN\n",
      "4        0.0   1.792748  0.166667    True      NaN\n",
      "..       ...        ...       ...     ...      ...\n",
      "145      0.0   6.747466  0.166667    True      NaN\n",
      "146      0.0   5.030282  0.166667    True      NaN\n",
      "147      0.0   2.340850  0.166667    True      NaN\n",
      "148      0.0   1.935907  0.166667    True      0.0\n",
      "149      0.0   2.560395  0.166667    True      NaN\n",
      "\n",
      "[150 rows x 5 columns]}}, 'training_iteration': 0, 'config': {'fit_cate_intercept': 1, 'mc_iters': 8, 'n_alphas': 777, 'n_alphas_cov': 7, 'tol': 0.0060376, 'max_iter': 1500}, 'config/fit_cate_intercept': 1, 'config/mc_iters': 8, 'config/n_alphas': 777, 'config/n_alphas_cov': 7, 'config/tol': 0.0060376, 'config/max_iter': 1500, 'experiment_tag': 'exp', 'time_total_s': 482.58409094810486}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Estimator: backdoor.econml.dml.SparseLinearDML\n",
      " erupt (validation): 6.584296\n",
      " qini (validation): -0.066673\n",
      " auc (validation): 0.537009\n",
      " ate (validation): 3.540849\n",
      " r_score (validation): 0.116589\n",
      "config: {'overall_model': AutoML(append_log=False, auto_augment=True, early_stop=False, ensemble=False,\n",
      "       estimator_list='auto', eval_method='auto', gpu_per_trial=0,\n",
      "       hpo_method='auto', keep_search_state=False, learner_selector='sample',\n",
      "       log_file_name='', log_training_metric=False, log_type='better',\n",
      "       max_iter=1000000, mem_thres=4294967296, metric='auto',\n",
      "       min_sample_size=10000, model_history=False, n_concurrent_trials=1,\n",
      "       n_jobs=-1, n_splits=5, pred_time_limit=1e-05, retrain_full=True,\n",
      "       sample=True, split_ratio=0.1, split_type='auto', starting_points={},\n",
      "       task='regression', time_budget=30, train_time_limit=inf, ...)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-14 14:46:50] {2145} WARNING - Time taken to find the best model is 71% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Estimator: backdoor.econml.metalearners.SLearner\n",
      " erupt (validation): 6.584296\n",
      " qini (validation): -0.045372\n",
      " auc (validation): 0.547148\n",
      " ate (validation): 3.748026\n",
      " r_score (validation): 0.114682\n",
      "config: {'propensity_model': DummyClassifier(), 'outcome_model': AutoML(append_log=False, auto_augment=True, early_stop=False, ensemble=False,\n",
      "       estimator_list='auto', eval_method='auto', gpu_per_trial=0,\n",
      "       hpo_method='auto', keep_search_state=False, learner_selector='sample',\n",
      "       log_file_name='', log_training_metric=False, log_type='better',\n",
      "       max_iter=1000000, mem_thres=4294967296, metric='auto',\n",
      "       min_sample_size=10000, model_history=False, n_concurrent_trials=1,\n",
      "       n_jobs=-1, n_splits=5, pred_time_limit=1e-05, retrain_full=True,\n",
      "       sample=True, split_ratio=0.1, split_type='auto', starting_points={},\n",
      "       task='regression', time_budget=30, train_time_limit=inf, ...)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[flaml.automl: 03-14 14:47:21] {2145} WARNING - Time taken to find the best model is 99% of the provided time budget and not all estimators' hyperparameter search converged. Consider increasing the time budget.\n",
      "[flaml.tune.tune: 03-14 14:47:21] {326} WARNING - Using CFO for search. To use BlendSearch, run: pip install flaml[blendsearch]\n",
      "[flaml.tune.tune: 03-14 14:47:21] {447} INFO - trial 1 config: {'min_propensity': 0.00039392081127942284, 'mc_iters': 5, 'n_estimators': 413, 'max_depth': 630, 'min_samples_split': 29, 'min_samples_leaf': 12, 'min_weight_fraction_leaf': 0.07449592798509735, 'max_features': 'sqrt', 'min_impurity_decrease': 5.266237972689596, 'max_samples': 0.12303482939899912, 'min_balancedness_tol': 0.23834773752057958, 'honest': 1, 'subforest_size': 7}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Estimator: backdoor.auto_causality.models.TransformedOutcome\n",
      " erupt (validation): 6.028015\n",
      " qini (validation): -0.024295\n",
      " auc (validation): 0.510177\n",
      " ate (validation): 3.535790\n",
      " r_score (validation): -0.637251\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "data_df = synth_ihdp()\n",
    "data_df, features_X, features_W, targets, treatment = preprocess_dataset(data_df)\n",
    "outcome = targets[0]\n",
    "\n",
    "# choose which estimators to fit\n",
    "estimator_list = [\"LinearDML\",\"SLearner\",\"TransformedOutcome\", \"ForestDRLearner\"]\n",
    "\n",
    "# init autocausality object with chosen metric to optimise\n",
    "ac = AutoCausality(\n",
    "    time_budget=180, \n",
    "    estimator_list=estimator_list, \n",
    "    metric=\"erupt\", \n",
    "    verbose=3,\n",
    "    components_verbose=2,\n",
    "    components_time_budget=30,\n",
    "    use_ray=False\n",
    ")\n",
    "\n",
    "# run autocausality\n",
    "myresults = ac.fit(data_df, treatment, outcome, features_W, features_X)\n",
    "\n",
    "# return best estimator\n",
    "print(f\"Best estimator: {ac.best_estimator}\")\n",
    "# config of best estimator:\n",
    "print(f\"best config: {ac.best_config}\")\n",
    "# best score:\n",
    "print(f\"best score: {ac.best_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e58dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac.train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d9a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome = targets[0]\n",
    "ac.full_scores[\"baseline\"]={\"estimator\": \"baseline\",\n",
    "                               \"outcome\": outcome,\n",
    "                              \"train\":{\"erupt\": ac.train_df[outcome].mean(),\n",
    "                                       \"ate\": ate(ac.train_df[treatment],ac.train_df[outcome])[0]},\n",
    "                              \"validation\":{\"erupt\": ac.test_df[outcome].mean(),\n",
    "                                      \"ate\": ate(ac.test_df[treatment],ac.test_df[outcome])[0]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4288a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "\n",
    "colors = ([matplotlib.colors.CSS4_COLORS['black']] +\n",
    "    list(matplotlib.colors.TABLEAU_COLORS) + [\n",
    "    matplotlib.colors.CSS4_COLORS['lime'],\n",
    "    matplotlib.colors.CSS4_COLORS['yellow'],\n",
    "    matplotlib.colors.CSS4_COLORS['pink']\n",
    "])\n",
    "\n",
    "v = ac.full_scores\n",
    "plt.figure(figsize = (7,5))\n",
    "plt.title(outcome)\n",
    "for (est, scr),col in zip(v.items(),colors):\n",
    "    sc = [scr['train']['erupt'], scr['validation']['erupt']]\n",
    "    crv = [scr['train']['ate'], scr['validation']['ate']]\n",
    "    plt.plot(sc, crv, color=col, marker=\"o\")\n",
    "    plt.scatter(sc[1:],crv[1:], c=col, s=120 )\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"ERUPT score\")\n",
    "    plt.ylabel(\"ATE\")\n",
    "    plt.legend(v.keys(),bbox_to_anchor=(1.04,1), borderaxespad=0)\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "scr = ac.full_scores[ac.best_estimator]\n",
    "intrp = scr['validation']['intrp']\n",
    "plt.figure(figsize=(15, 7))\n",
    "try: \n",
    "    feature_names = intrp.feature_names\n",
    "except:\n",
    "    feature_names = features_X + [ w for w in features_W if w not in features_X]\n",
    "intrp.plot(feature_names=intrp.feature_names, fontsize=10)\n",
    "#         intrp.plot( fontsize=10)\n",
    "plt.title(f\"{ac.best_estimator}_{outcome}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d544dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add SHAP plots!\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# and now let's visualize feature importances!\n",
    "from auto_causality.shap import shap_values\n",
    "\n",
    "# Shapley values calculation can be slow so let's subsample\n",
    "this_df = ac.test_df.sample(100)\n",
    "\n",
    "wanted = [\"CausalForestDML\"]#,\"ForestDRLearner\",\"DirectUpliftDoWhyWrapper\"]#,\"CausalForestDML\",]\n",
    "\n",
    "scr = ac.full_scores[ac.best_estimator]\n",
    "print(outcome, ac.best_estimator)\n",
    "est = ac.estimates[ac.best_estimator]\n",
    "shaps = shap_values(est, this_df)\n",
    "\n",
    "plt.title(outcome + '_' + ac.best_estimator.split('.')[-1])\n",
    "shap.summary_plot(shaps, this_df[est.estimator._effect_modifier_names])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
