{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a34f30c6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ERUPT under simulated random assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c37a7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # suppress sklearn deprecation warnings for now..\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# the below checks for whether we run dowhy, causaltune, and FLAML from source\n",
    "root_path = root_path = os.path.realpath('../..')\n",
    "try:\n",
    "    import causaltune\n",
    "except ModuleNotFoundError:\n",
    "    sys.path.append(os.path.join(root_path, \"causaltune\"))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from causaltune.erupt import DummyPropensity, ERUPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af5333b0",
   "metadata": {},
   "source": [
    "## Creating synthetic data for the experiment with fully random assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0133aede-f086-4062-9899-360ab2ca2f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>T1</th>\n",
       "      <th>Y1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.528828</td>\n",
       "      <td>0</td>\n",
       "      <td>1.472417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.712731</td>\n",
       "      <td>0</td>\n",
       "      <td>0.774130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.796902</td>\n",
       "      <td>1</td>\n",
       "      <td>2.005292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.788617</td>\n",
       "      <td>1</td>\n",
       "      <td>2.038432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.130933</td>\n",
       "      <td>1</td>\n",
       "      <td>2.099868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X  T1        Y1\n",
       "0  0.528828   0  1.472417\n",
       "1  0.712731   0  0.774130\n",
       "2  0.796902   1  2.005292\n",
       "3  0.788617   1  2.038432\n",
       "4  0.130933   1  2.099868"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create some synthetic data\n",
    "n=10000\n",
    "\n",
    "# Let's create a dataset with a single feature\n",
    "df = pd.DataFrame({\"X\": np.random.uniform(size=n)})\n",
    "\n",
    "# Now let's create a response-to-treatment function that correlates with the feature\n",
    "def outcome(x: np.ndarray, treatment: np.ndarray) -> np.ndarray:\n",
    "    return 2*np.random.uniform(size=len(x)) + x*(treatment == 1)\n",
    "\n",
    "# Let's consider a fully random treatment\n",
    "df[\"T1\"] = np.random.randint(0, 2, size=n)\n",
    "# and simulate the corresponing experiment outcomes\n",
    "df[\"Y1\"] = outcome(df[\"X\"], df[\"T1\"])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60d8ef9d-9b7e-44c4-abd6-7eb8e00dbda5",
   "metadata": {},
   "source": [
    "## Creating synthetic data for the experiment with biased assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69888516-0f22-4e68-ae16-9fe1357ed86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>T1</th>\n",
       "      <th>Y1</th>\n",
       "      <th>p</th>\n",
       "      <th>T2</th>\n",
       "      <th>p_of_actual</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.528828</td>\n",
       "      <td>0</td>\n",
       "      <td>1.472417</td>\n",
       "      <td>0.764414</td>\n",
       "      <td>1</td>\n",
       "      <td>0.764414</td>\n",
       "      <td>1.360950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.712731</td>\n",
       "      <td>0</td>\n",
       "      <td>0.774130</td>\n",
       "      <td>0.856365</td>\n",
       "      <td>1</td>\n",
       "      <td>0.856365</td>\n",
       "      <td>2.104692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.796902</td>\n",
       "      <td>1</td>\n",
       "      <td>2.005292</td>\n",
       "      <td>0.898451</td>\n",
       "      <td>1</td>\n",
       "      <td>0.898451</td>\n",
       "      <td>2.144687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.788617</td>\n",
       "      <td>1</td>\n",
       "      <td>2.038432</td>\n",
       "      <td>0.894308</td>\n",
       "      <td>1</td>\n",
       "      <td>0.894308</td>\n",
       "      <td>1.983497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.130933</td>\n",
       "      <td>1</td>\n",
       "      <td>2.099868</td>\n",
       "      <td>0.565467</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565467</td>\n",
       "      <td>0.942264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X  T1        Y1         p  T2  p_of_actual        Y2\n",
       "0  0.528828   0  1.472417  0.764414   1     0.764414  1.360950\n",
       "1  0.712731   0  0.774130  0.856365   1     0.856365  2.104692\n",
       "2  0.796902   1  2.005292  0.898451   1     0.898451  2.144687\n",
       "3  0.788617   1  2.038432  0.894308   1     0.894308  1.983497\n",
       "4  0.130933   1  2.099868  0.565467   1     0.565467  0.942264"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's consider another experiment on the same population, but with \n",
    "# treatment assignment that's biased by the feature, because we believe that \n",
    "# customers with higher values of the feature will be more responsive to the treatment\n",
    "\n",
    "df[\"p\"] = 0.5+0.5*df[\"X\"] # probability of binary treatment being applied\n",
    "df[\"T2\"] = (np.random.rand(len(df)) <df[\"p\"]).astype(int) # sample with that propensity\n",
    "\n",
    "# We really only need the ex ante probability of the treatment that actually was applied\n",
    "# This will work exactly the same way in a multi-treatment case\n",
    "df[\"p_of_actual\"] = df[\"p\"]*df[\"T2\"] + (1-df[\"p\"])*(1-df[\"T2\"])\n",
    "\n",
    "# Now let's evaluate the outcome for this experiment\n",
    "\n",
    "df[\"Y2\"] = outcome(df[\"X\"], df[\"T2\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fae937-cafd-4c26-9b7e-f926714543d3",
   "metadata": {},
   "source": [
    "## Create a simple function to calculate estimated outcomes for alternate assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8afee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a simple function that takes the actual experiment data and the hypothetical policy,\n",
    "# and returns the average outcome under the hypothetical policy\n",
    "def evaluate_erupt(actual_propensity: pd.Series,\n",
    "                   actual_treatment: pd.Series,\n",
    "                   actual_outcome: pd.Series,\n",
    "                   hypothetical_policy: pd.Series) -> float:\n",
    "    \n",
    "    # define a dummy propensity model that will simply return the propensities when calling predict_proba\n",
    "    propensity_model = DummyPropensity(p=actual_propensity, treatment=actual_treatment)\n",
    "\n",
    "    # obtain ERUPT value under hypothetical policy\n",
    "    e = ERUPT(treatment_name=\"T\", propensity_model=propensity_model)\n",
    "    return e.score(df=pd.DataFrame({\"T\": np.array(actual_treatment)}), outcome=actual_outcome, policy=hypothetical_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02fea3e-e281-4dca-9197-47580cb0fd70",
   "metadata": {},
   "source": [
    "## Estimate random assignment outcome from biased assignment experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c061fe2-c56c-4be6-8fa7-b63546faebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average outcome of the actual biased assignment experiment: 1.4186689330175457\n",
      "Estimated outcome of random assignment: 1.2463633587748948\n",
      "Average outcome of the actual random assignment experiment: 1.2462788233315247\n"
     ]
    }
   ],
   "source": [
    "# Let's use data from biased assignment experiment to estimate the average effect of fully random assignment\n",
    "est = evaluate_erupt(actual_propensity=df[\"p_of_actual\"], \n",
    "                     actual_treatment=df[\"T2\"],\n",
    "                     actual_outcome=df[\"Y2\"],\n",
    "                     hypothetical_policy=df[\"T1\"])\n",
    "\n",
    "\n",
    "print(\"Average outcome of the actual biased assignment experiment:\", df[\"Y2\"].mean())\n",
    "print(\"Estimated outcome of random assignment:\", est)\n",
    "print(\"Average outcome of the actual random assignment experiment:\",  df[\"Y1\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451d17f8-b45c-4c60-9746-e1a9e9fac2c8",
   "metadata": {},
   "source": [
    "## Estimate biased assignment outcome from random assignment experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7476b38c-6912-478e-93dd-9954a75e8bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average outcome of the actual random assignment experiment: 1.2462788233315247\n",
      "Estimated outcome of biased assignment: 1.4054069453155034\n",
      "Average outcome of the actual biased assignment experiment: 1.4186689330175457\n"
     ]
    }
   ],
   "source": [
    "# Conversely, we can take the outcome of the fully random test and use it to estimate what the outcome of the biased assignment would have been\n",
    "\n",
    "# Let's use data from biased assignment experiment to estimate the average effect of fully random assignment\n",
    "hypothetical_policy = df[\"T2\"]\n",
    "est = evaluate_erupt(actual_propensity=0.5*pd.Series(np.ones(len(df))), \n",
    "                     actual_treatment=df[\"T1\"],\n",
    "                     actual_outcome=df[\"Y1\"],\n",
    "                     hypothetical_policy= df[\"T2\"])\n",
    "\n",
    "print(\"Average outcome of the actual random assignment experiment:\", df[\"Y1\"].mean())\n",
    "print(\"Estimated outcome of biased assignment:\", est)\n",
    "print(\"Average outcome of the actual biased assignment experiment:\",  df[\"Y2\"].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a54530bf",
   "metadata": {},
   "source": [
    "For more details on the math behind ERUPT, consult [Hitsch and Misra (2018)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3111957), who call it policy value. Note also that we assume that treatment takes integer values from 0 to n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99cd00-f2af-4cfa-bd79-5c2a4cbc1828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-causaltune]",
   "language": "python",
   "name": "conda-env-.conda-causaltune-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
