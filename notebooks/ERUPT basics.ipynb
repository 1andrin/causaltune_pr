{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a34f30c6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Combining exploration and exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab88c3f-d9d9-4d91-809d-afa60d62c521",
   "metadata": {},
   "source": [
    "Have you ever wanted to conduct A/B/N tests in a case where you had a prior opinion on which variant could be better suited to which customer?\n",
    "\n",
    "Were you torn between exploration (learning from A/B/N test results) and exploitation (showing each customer the variant you already think is most suited for them)?\n",
    "\n",
    "Did you know that you can do both at once? \n",
    "\n",
    "By making variant assignments random but biased towards what you think will work best for each customer, you can get the best of both worlds. You can observe the actual impact (for example, conversion rates) of your biased assignments and at the same time calculate, *from the same experiment*, an unbiased, model-free estimate of what the conversion rate for a purely random assignment would have been, using a mathematical technique called ERUPT or policy value.  This means that beyond the usual learnings from an A/B/N test you now have a precise, unbiased estimate of the benefit that biasing the assignment has brought. \n",
    "\n",
    "Suppose you're not really sure about your prior beliefs. In that case, you can also turn this around: run a fully randomized experiment, then use ERUPT to calculate from that experiment an unbiased estimate of what the impact of _any other assignment policy_ would have been! \n",
    "\n",
    "Summing up, if your actual variant assignment policy is at all stochastic, whether fully random or with a probability depending on customer's characteristics, after running the experiment you can use ERUPT to get an unbiased estimate of what the outcome of ANY OTHER ASSIGNMENT POLICY for the same experiment would have been! \n",
    "\n",
    "This notebook shows you how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c37a7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # suppress sklearn deprecation warnings for now..\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# the below checks for whether we run dowhy, causaltune, and FLAML from source\n",
    "root_path = root_path = os.path.realpath('../..')\n",
    "try:\n",
    "    import causaltune\n",
    "except ModuleNotFoundError:\n",
    "    sys.path.append(os.path.join(root_path, \"causaltune\"))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from causaltune.erupt import DummyPropensity, ERUPT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af5333b0",
   "metadata": {},
   "source": [
    "## Creating synthetic data for the experiment with fully random assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0133aede-f086-4062-9899-360ab2ca2f3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>T1</th>\n",
       "      <th>Y1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.528828</td>\n",
       "      <td>0</td>\n",
       "      <td>1.472417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.712731</td>\n",
       "      <td>0</td>\n",
       "      <td>0.774130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.796902</td>\n",
       "      <td>1</td>\n",
       "      <td>2.005292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.788617</td>\n",
       "      <td>1</td>\n",
       "      <td>2.038432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.130933</td>\n",
       "      <td>1</td>\n",
       "      <td>2.099868</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X  T1        Y1\n",
       "0  0.528828   0  1.472417\n",
       "1  0.712731   0  0.774130\n",
       "2  0.796902   1  2.005292\n",
       "3  0.788617   1  2.038432\n",
       "4  0.130933   1  2.099868"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create some synthetic data\n",
    "n=10000\n",
    "\n",
    "# Let's create a dataset with a single feature\n",
    "df = pd.DataFrame({\"X\": np.random.uniform(size=n)})\n",
    "\n",
    "# Now let's create a response-to-treatment function that correlates with the feature\n",
    "def outcome(x: np.ndarray, treatment: np.ndarray) -> np.ndarray:\n",
    "    return 2*np.random.uniform(size=len(x)) + x*(treatment == 1)\n",
    "\n",
    "# Let's consider a fully random treatment\n",
    "df[\"T1\"] = np.random.randint(0, 2, size=n)\n",
    "# and simulate the corresponing experiment outcomes\n",
    "df[\"Y1\"] = outcome(df[\"X\"], df[\"T1\"])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60d8ef9d-9b7e-44c4-abd6-7eb8e00dbda5",
   "metadata": {},
   "source": [
    "## Creating synthetic data for the experiment with biased assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69888516-0f22-4e68-ae16-9fe1357ed86a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>T1</th>\n",
       "      <th>Y1</th>\n",
       "      <th>p</th>\n",
       "      <th>T2</th>\n",
       "      <th>p_of_actual</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.528828</td>\n",
       "      <td>0</td>\n",
       "      <td>1.472417</td>\n",
       "      <td>0.764414</td>\n",
       "      <td>1</td>\n",
       "      <td>0.764414</td>\n",
       "      <td>1.360950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.712731</td>\n",
       "      <td>0</td>\n",
       "      <td>0.774130</td>\n",
       "      <td>0.856365</td>\n",
       "      <td>1</td>\n",
       "      <td>0.856365</td>\n",
       "      <td>2.104692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.796902</td>\n",
       "      <td>1</td>\n",
       "      <td>2.005292</td>\n",
       "      <td>0.898451</td>\n",
       "      <td>1</td>\n",
       "      <td>0.898451</td>\n",
       "      <td>2.144687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.788617</td>\n",
       "      <td>1</td>\n",
       "      <td>2.038432</td>\n",
       "      <td>0.894308</td>\n",
       "      <td>1</td>\n",
       "      <td>0.894308</td>\n",
       "      <td>1.983497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.130933</td>\n",
       "      <td>1</td>\n",
       "      <td>2.099868</td>\n",
       "      <td>0.565467</td>\n",
       "      <td>1</td>\n",
       "      <td>0.565467</td>\n",
       "      <td>0.942264</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          X  T1        Y1         p  T2  p_of_actual        Y2\n",
       "0  0.528828   0  1.472417  0.764414   1     0.764414  1.360950\n",
       "1  0.712731   0  0.774130  0.856365   1     0.856365  2.104692\n",
       "2  0.796902   1  2.005292  0.898451   1     0.898451  2.144687\n",
       "3  0.788617   1  2.038432  0.894308   1     0.894308  1.983497\n",
       "4  0.130933   1  2.099868  0.565467   1     0.565467  0.942264"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's consider another experiment on the same population, but with \n",
    "# treatment assignment that's biased by the feature, because we believe that \n",
    "# customers with higher values of the feature will be more responsive to the treatment\n",
    "\n",
    "df[\"p\"] = 0.5+0.5*df[\"X\"] # probability of binary treatment being applied\n",
    "df[\"T2\"] = (np.random.rand(len(df)) <df[\"p\"]).astype(int) # sample with that propensity\n",
    "\n",
    "# We really only need the ex ante probability of the treatment that actually was applied\n",
    "# This will work exactly the same way in a multi-treatment case\n",
    "df[\"p_of_actual\"] = df[\"p\"]*df[\"T2\"] + (1-df[\"p\"])*(1-df[\"T2\"])\n",
    "\n",
    "# Now let's evaluate the outcome for this experiment\n",
    "\n",
    "df[\"Y2\"] = outcome(df[\"X\"], df[\"T2\"])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fae937-cafd-4c26-9b7e-f926714543d3",
   "metadata": {},
   "source": [
    "## Create a simple function to calculate estimated outcomes for alternate assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8afee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a simple function that takes the actual experiment data and the hypothetical policy,\n",
    "# and returns the average outcome under the hypothetical policy\n",
    "def evaluate_erupt(actual_propensity: pd.Series,\n",
    "                   actual_treatment: pd.Series,\n",
    "                   actual_outcome: pd.Series,\n",
    "                   hypothetical_policy: pd.Series) -> float:\n",
    "    \n",
    "    # define a dummy propensity model that will simply return the propensities when calling predict_proba\n",
    "    propensity_model = DummyPropensity(p=actual_propensity, treatment=actual_treatment)\n",
    "\n",
    "    # obtain ERUPT value under hypothetical policy\n",
    "    e = ERUPT(treatment_name=\"T\", propensity_model=propensity_model)\n",
    "    return e.score(df=pd.DataFrame({\"T\": np.array(actual_treatment)}), outcome=actual_outcome, policy=hypothetical_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02fea3e-e281-4dca-9197-47580cb0fd70",
   "metadata": {},
   "source": [
    "## Estimate random assignment outcome from biased assignment experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c061fe2-c56c-4be6-8fa7-b63546faebfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average outcome of the actual biased assignment experiment: 1.4186689330175457\n",
      "Estimated outcome of random assignment: 1.2463633587748948\n",
      "Average outcome of the actual random assignment experiment: 1.2462788233315247\n"
     ]
    }
   ],
   "source": [
    "# Let's use data from biased assignment experiment to estimate the average effect of fully random assignment\n",
    "est = evaluate_erupt(actual_propensity=df[\"p_of_actual\"], \n",
    "                     actual_treatment=df[\"T2\"],\n",
    "                     actual_outcome=df[\"Y2\"],\n",
    "                     hypothetical_policy=df[\"T1\"])\n",
    "\n",
    "\n",
    "print(\"Average outcome of the actual biased assignment experiment:\", df[\"Y2\"].mean())\n",
    "print(\"Estimated outcome of random assignment:\", est)\n",
    "print(\"Average outcome of the actual random assignment experiment:\",  df[\"Y1\"].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451d17f8-b45c-4c60-9746-e1a9e9fac2c8",
   "metadata": {},
   "source": [
    "## Estimate biased assignment outcome from random assignment experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7476b38c-6912-478e-93dd-9954a75e8bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average outcome of the actual random assignment experiment: 1.2462788233315247\n",
      "Estimated outcome of biased assignment: 1.4054069453155034\n",
      "Average outcome of the actual biased assignment experiment: 1.4186689330175457\n"
     ]
    }
   ],
   "source": [
    "# Conversely, we can take the outcome of the fully random test and use it to estimate what the outcome of the biased assignment would have been\n",
    "\n",
    "# Let's use data from biased assignment experiment to estimate the average effect of fully random assignment\n",
    "hypothetical_policy = df[\"T2\"]\n",
    "est = evaluate_erupt(actual_propensity=0.5*pd.Series(np.ones(len(df))), \n",
    "                     actual_treatment=df[\"T1\"],\n",
    "                     actual_outcome=df[\"Y1\"],\n",
    "                     hypothetical_policy= df[\"T2\"])\n",
    "\n",
    "print(\"Average outcome of the actual random assignment experiment:\", df[\"Y1\"].mean())\n",
    "print(\"Estimated outcome of biased assignment:\", est)\n",
    "print(\"Average outcome of the actual biased assignment experiment:\",  df[\"Y2\"].mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a54530bf",
   "metadata": {},
   "source": [
    "For more details on the math behind ERUPT, consult [Hitsch and Misra (2018)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3111957), who call it policy value. Note also that we assume that treatment takes integer values from 0 to n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99cd00-f2af-4cfa-bd79-5c2a4cbc1828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-causaltune]",
   "language": "python",
   "name": "conda-env-.conda-causaltune-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
